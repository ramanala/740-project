1 Introduction: Need to write the whole thing
- Emphasis point: Why is application-level consistency different than fs
consistency?
- tool allows developers to be "proactive" rather than "react"
- tell the story of delayed allocation and ext4?
- but how do I weave it in?

2 Example section:
- Move section 7 forward
- Add new compact figures, see if it makes sense.
- Merge some definitions into this section
    - application consistency - some of this will be in the intro as well
    - update protocol

- Why tool helps:
    - filter
    - readable
    - dependencies
    - Alternative: talk to developers.

3 Followed by an explanation of why it is that update protocols are hard to
write and test?
- bring in thanu's point about it being hard to test.
- Emphasis point: *Why* do applications have different protocols?  To
help make this point, I'd move  the point about "protocol complexity"
to the beginning of this section.  Maybe you want to say something
specific about fsync performance cost.

If we talk about the crash states, Elaboration: how do you this analysis?  (you
just consider all the permutations of these system calls? or, do you need to
say the details will be described later?)  For clarification, can you show in
Table 1 the combination of system calls that lead to each state?  
- This is a little hard because there are 2 things going on here: each system
call can be persisted or not, and each system call has different intermediate
states that can be persisted or not.

4 Persistence Properties
- Wording: I like the paragraph on the intuition, but I don't think you
want to conclude with "the *manner* in which I/O becomes persistent on
disk".  Maybe just say, "it is enough for persistency properties to
simply capture *which* I/O becomes persistent on disk".

- Make the operations considerably more high level. We don't want a lot of special cases.
- Separate out the properties from the operation notation.
- Other minor things in Andrea's comments.
- Removing durability as a persistence property, and all attached description.

The section about estimating the properties doesn't seem very interesting. Make
it a para inside the "File Systems Results" section.

- You need to give some overview about your results when you introduce
Table 3.   What can you generally say about each file system?  You
need some conclusion for this section.

5 The Tool - we still need a good name for this
- 4.1.1 File System Model
I feel like you need a higher-level description of your model.   Why
is your model realistic and/or practical?
- The dependencies info is intuitive, but the atomicity model is not.
Why are links atomic and unlinks are not?

Since Detector starts with a system call trace, I think you need to
explain how your model maps system calls to logical operations and
then to micro operations.  I would describe your model from the top
down.

I think you can use Table 4 much earlier to explain how the model goes
from logical operations to micro ops.  (Maybe you should add fsync to
this table -- though I know it corresponds to dependencies and not to
micro-code.)  And, maybe you can add more information to the Table to
show op sizes (e.g., of overwrites and appends)?   Should truncate
also be in this table?

4.2.1 Currently, you explain micro-code as if the existence of those 4
ops is self-evident -- you need to give some intuition for why those
make sense as the base primitives.  (But, again, I would describe the
model from the top down.)

- Change back to static and dynamic vulnerabilities. 

6 Evaluation

7 Vul study
- Add the setup
- Add the experience part.
    - Add a subpart talking about interactions with developers?
- Can make the current parts more interesting with input from Thanu.

 
8 Evaluating new file-system design
- Hypothesis

9 Related Work
- explode
- woodpecker
- verify (model checking?)
- explicating sdk
- Tyler's study 

10 Conclusion 

